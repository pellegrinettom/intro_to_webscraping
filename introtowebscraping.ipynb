{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro To Web Scraping in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Never Sleeps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The total volume of data created and stored reached 64.2 ZB in 2020 and is expected to grow to more than 180 zettabytes by 2025 ([Statista, 2021](https://www.statista.com/statistics/871513/worldwide-data-created/))\n",
    "\n",
    "- Every minute, users worldwide conduct 5.9 million searches on Google, share 437,000 Tweets, and upload 500 hours of videos on YouTube. ([DOMO, 2022](https://www.domo.com/data-never-sleeps))\n",
    "\n",
    "-  A considerable part of this data is **publicly available online** and offers valuable insights for economic and business research (e.g., quantify consumption, measure prices, track consumer behaviors, etc.)\n",
    "\n",
    "- Example: Growing use of web data across the top 5 marketing journals ([Boegershausen et al., 2022](https://journals.sagepub.com/doi/pdf/10.1177/00222429221100750))\n",
    "\n",
    "![convert notebook to web app](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/Boegershausen22.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Online Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Problem**: Gathering data dispersed throughout the Internet manually is usually unfeasible or incredibly time-consuming. \n",
    "\n",
    "- **Solution**: Researchers can automate the data collection process with the use of:\n",
    "\n",
    "    - **Application Programming Interfaces (APIs)**:  often unavailable or expensive\n",
    "\n",
    "    - **Web Scraping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Web scraping** is the automated process of extracting information from websites using specialized programs or scripts.\n",
    "\n",
    "- Key advantages:\n",
    "\n",
    "    - **Enhanced data accessibility**: no gatekeepers, cheaper and faster than traditional data collection methods\n",
    "\n",
    "    - **Flexibility in data collection**: more control over various parameters, like frequency and granularity.\n",
    "\n",
    "    - **Simplicity of Implementation**: powerful libraries available in many programming languages (e.g. Python, R, Julia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Raval (2023)](https://deveshraval.github.io/buyBox.pdf): Scraped information for hundreds of thousands of products on Amazon and used it to investigate what drives the BuyBox assignment.\n",
    "\n",
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/raval.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Decarolis, Rovigatti (2021)](https://www.aeaweb.org/articles?id=10.1257/aer.20190811) scraped data on nearly 40 million Google keyword auctions from Semrush.com and applied ML techniques to define advertising markets.\n",
    "\n",
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/decarolisrovigatti.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our goals for today"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Familiarize with the basics of web scraping\n",
    "\n",
    "2. Practice with main libraries for web scraping in Python\n",
    "\n",
    "3. Discuss how to approach web scraping in a responsible and smart way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The requests package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTTP requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **HTTP (Hypertext Transfer Protocol)** is the protocol used to transfer data over the web.\n",
    "\n",
    "- It operates on a request-response model, where:\n",
    "    1.  a client (e.g., a web browser) sends a request to a server\n",
    "    2.  the server responds with the requested information.\n",
    "\n",
    "- It defines various methods that indicate the action the client wants to perform on a resource. Most useful for web scraping: \n",
    "    - the **GET** method retrieves data\n",
    "    - the **POST** method submits data \n",
    "\n",
    "- [```requests```](https://pypi.org/project/requests/) is a library that allows to handle HTTP requests in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 200\n",
      "{'userId': 1, 'id': 1, 'title': 'quidem molestiae enim'}\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE OF GET REQUEST\n",
    "import requests\n",
    "\n",
    "# GET request example\n",
    "response = requests.get(\n",
    "    \"https://jsonplaceholder.typicode.com/albums\"\n",
    "    )\n",
    "\n",
    "# Check the status code (200 is good, 400-ish is bad)\n",
    "print(\"Status code:\", response.status_code)\n",
    "\n",
    "# the content of the response can be formatted as\n",
    "# a list of dictionaries using the .json() method\n",
    "print(response.json()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userId': 1, 'title': 'my title', 'id': 101}\n"
     ]
    }
   ],
   "source": [
    "# EXAMPLE OF POST REQUEST\n",
    "import json\n",
    "\n",
    "# Define the headers for the POST request\n",
    "headers = {\n",
    "    'Content-type': 'application/json; charset=UTF-8',\n",
    "}\n",
    "\n",
    "# Define the data we want to attach (i.e. the payload)\n",
    "data = { 'userId': 1, 'title': 'my title'}\n",
    "\n",
    "# Send the POST request\n",
    "response = requests.post(\n",
    "    'https://jsonplaceholder.typicode.com/posts',\n",
    "data=json.dumps(data), headers=headers)\n",
    "\n",
    "# Print the newly added data\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Use-Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cover three use-cases of the requests package:\n",
    "\n",
    "- HTML-based Web Scraping\n",
    "\n",
    "- XHR-based Web Scraping\n",
    "\n",
    "- Automated files download\n",
    "\n",
    "But there is much more: \n",
    "\n",
    "- Authentication and logins\n",
    "\n",
    "- Use of official APIs\n",
    "\n",
    "- IP Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HTML-based Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML: The Language of the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you order something online...\n",
    "\n",
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/webpage.png?raw=true)\n",
    "\n",
    "... when it arrives\n",
    "\n",
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/htmlsource.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **HTML (HyperText Markup Language)** is the standard language used for creating and structuring content of webpages.\n",
    "\n",
    "- HTML documents are composed of **HTML elements**, which are defined by a start tag, some content, and an end tag.\n",
    "\n",
    "    ```<h1>My First Heading</h1>```\n",
    "\n",
    "- **HTML tags** are like keywords which define how web browsers will format and display the content. \n",
    "\n",
    "    - Some common tags are ```li``` for lists, ```a``` for links, and ```table``` for tables.\n",
    " [See more tags](https://www.w3schools.com/tags/ref_byfunc.asp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML is a beautiful soup!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[```beautifulsoup```](https://pypi.org/project/beautifulsoup4/) is a Python package that\n",
    "can parse HTML content and extract information from its elements.\n",
    "\n",
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/beautifulsoup.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use Developer Tools "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/identifyHTML.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bookstore Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define the target URL\n",
    "target_url = \"https://books.toscrape.com/\"\n",
    "\n",
    "# Request the page\n",
    "page = requests.get(target_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<!--[if lt IE '\n"
     ]
    }
   ],
   "source": [
    "# Inspect the content of the page: it's the HTML source of the webpage\n",
    "print(page.content[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the page\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products: 20\n"
     ]
    }
   ],
   "source": [
    "# Explore the HTML from the dev tools in Chrome\n",
    "# and find the relevant elements\n",
    "products = soup.find_all(\"article\", class_=\"product_pod\")\n",
    "print('Number of products:', len(products))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   title  price\n",
      "0                   A Light in the Attic  51.77\n",
      "1                     Tipping the Velvet  53.74\n",
      "2                             Soumission  50.10\n",
      "3                          Sharp Objects  47.82\n",
      "4  Sapiens: A Brief History of Humankind  54.23\n"
     ]
    }
   ],
   "source": [
    "# Extract product title and price\n",
    "titles = []\n",
    "prices = []\n",
    "\n",
    "for p in products:\n",
    "    titles.append(p.find(\"h3\").find(\"a\")[\"title\"])\n",
    "    prices.append(\n",
    "            p.find(\"p\", class_=\"price_color\").text[1:])\n",
    "\n",
    "\n",
    "# Build the dataframe\n",
    "df = pd.DataFrame(\n",
    "    {\"title\": titles, \"price\": prices}\n",
    ")\n",
    "df['price'] = df['price'].astype(float)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a folder and save the dataframe as a CSV file\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "\n",
    "# Store the dataframe as a CSV file\n",
    "df.to_csv(\"data/books_example.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now scale up our scraping example to multiple book categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Travel', 'Mystery', 'Historical Fiction']\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "import random\n",
    "\n",
    "# Get the full list of book categories\n",
    "categories = soup.find(\"div\", class_=\"side_categories\")\n",
    "\n",
    "# Get the full list of book categories\n",
    "ul_element = soup.find(\"ul\",\n",
    "                       class_ = 'nav nav-list').find(\"ul\")\n",
    "li_elements = ul_element.find_all(\"li\")\n",
    "categories = [category.text.strip()\n",
    "                for category in li_elements]\n",
    "\n",
    "# # Print the first four categories\n",
    "print(categories[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:05,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 51 entries, 0 to 19\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   title     51 non-null     object \n",
      " 1   price     51 non-null     float64\n",
      " 2   rating    51 non-null     object \n",
      " 3   category  51 non-null     object \n",
      "dtypes: float64(1), object(3)\n",
      "memory usage: 2.0+ KB\n",
      "None\n",
      "                                               title  price rating category\n",
      "0                            It's Only the Himalayas  45.17    Two   Travel\n",
      "1  Full Moon over Noah’s Ark: An Odyssey to Mount...  49.43   Four   Travel\n",
      "2  See America: A Celebration of Our National Par...  48.87  Three   Travel\n",
      "3  Vagabonding: An Uncommon Guide to the Art of L...  36.94    Two   Travel\n",
      "4                               Under the Tuscan Sun  37.33  Three   Travel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the category dataframes\n",
    "df_list = []\n",
    "\n",
    "# Define the base url\n",
    "base_url = \"https://books.toscrape.com/catalogue/category/books/\"\n",
    "\n",
    "# Scrape the first page for the first 3 categories\n",
    "for category, idx in tqdm(zip(categories[:3], range(2,5))):\n",
    "\n",
    "    # Wait for 1 to 3 seconds before each request\n",
    "    # (to avoid overloading the server)\n",
    "    sleep(random.randint(1, 3))\n",
    "\n",
    "    # Build the category url\n",
    "    category_url = (\n",
    "        base_url\n",
    "        + category.lower().replace(' ', '-')\n",
    "        + '_'\n",
    "        + str(idx)\n",
    "        + '/index.html'\n",
    "    )\n",
    "\n",
    "    # Request the page\n",
    "    category_page = requests.get(category_url)\n",
    "\n",
    "    # Check the status code\n",
    "    if category_page.status_code != 200:\n",
    "        print(\"Error with category:\", category)\n",
    "        continue\n",
    "\n",
    "    # Parse the page\n",
    "    category_soup = BeautifulSoup(category_page.content, \"html.parser\")\n",
    "\n",
    "    # Find the relevant tag\n",
    "    category_products = category_soup.find_all(\"article\", class_=\"product_pod\")\n",
    "\n",
    "    # Extract product title\n",
    "    category_titles = [p.find(\"h3\").find(\"a\")[\"title\"] for p in category_products]\n",
    "\n",
    "    # Extract product price\n",
    "    category_prices = [\n",
    "        p.find(\"p\", class_=\"price_color\").text[1:] for p in category_products\n",
    "    ]\n",
    "\n",
    "    # Extract product rating\n",
    "    category_ratings = [\n",
    "        p.find(\"p\", class_=\"star-rating\")[\"class\"][1] for p in category_products\n",
    "    ]\n",
    "\n",
    "    # Build the dataframe\n",
    "    category_df = pd.DataFrame(\n",
    "        {\"title\": category_titles, \"price\": category_prices, \"rating\": category_ratings}\n",
    "    )\n",
    "    category_df[\"category\"] = category\n",
    "    category_df['price'] = category_df['price'].astype(float)\n",
    "    df_list.append(category_df)\n",
    "\n",
    "# Concatenate all the dataframes\n",
    "categories_df = pd.concat(df_list)\n",
    "print(categories_df.info())\n",
    "print(categories_df.head())\n",
    "categories_df.to_csv(\"data/books_example_categories.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCERCISE\n",
    "# Scrape the first 10 pages of books from the first 10 categories\n",
    "# Hint: You can get the number of pages from the bottom of the webpage\n",
    "# category_pages = category_soup.find('li', class_='current').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML-based Web Scraping: S&W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strengths**:\n",
    "\n",
    "- Easy to implement and debug\n",
    "\n",
    "- Faster than other approaches\n",
    "\n",
    "- Scalable\n",
    "\n",
    "\n",
    "**Weaknesses**:\n",
    "\n",
    "- Could be unstable over time (source code changes)\n",
    "\n",
    "- Cannot handle dynamically loaded content\n",
    "\n",
    "- Cannot interact with the webpage (click, type, scroll, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. XHR-based Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webpages can be thought of as a combination of structure and content.\n",
    "\n",
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/contentstructure.png?raw=true)\n",
    "\n",
    "XHR-based web scraping allows to request the content directly, without caring about the structure.\n",
    "\n",
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/split_content_structure.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entering from the Backdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This technique aims to intercept the [XHR requests](https://www.w3schools.com/xml/xml_http.asp) that the web page sends in the background to update its content without reloading the structure of the page.\n",
    "\n",
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/page_xhr_server.jpg?raw=true)\n",
    "\n",
    "[OPEN MOVIES EXAMPLE](https://www.scrapethissite.com/pages/ajax-javascript/#2015)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitoring HXR Requests in Chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/xhrmonitoring.png?raw=true)\n",
    "\n",
    "\n",
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/xhrmonitoring2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oscar Winning Films Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Spotlight  ', 'year': 2015, 'awards': 2, 'nominations': 6, 'best_picture': True}\n"
     ]
    }
   ],
   "source": [
    "# Define the URL\n",
    "base_url = 'https://www.scrapethissite.com/pages/ajax-javascript/'\n",
    "parameters = '?ajax=true&year=2015'\n",
    "url = base_url + parameters\n",
    "\n",
    "# Read the headers from the header/headers.json file\n",
    "with open('files/headers/header.json', 'r') as f:\n",
    "    headers = json.load(f)\n",
    "\n",
    "# Send the GET request\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Format the response as a list of dictionaries\n",
    "response_json = response.json()\n",
    "print(response_json[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 title  year  awards  nominations best_picture\n",
      "0          Spotlight    2015       2            6         True\n",
      "1  Mad Max: Fury Road   2015       6           10          NaN\n",
      "2      The Revenant     2015       3           12          NaN\n",
      "3      Bridge of Spies  2015       1            6          NaN\n",
      "4      The Big Short    2015       1            5          NaN\n"
     ]
    }
   ],
   "source": [
    "# Format the response as a dataframe\n",
    "oscars_df = pd.DataFrame(response_json)\n",
    "print(oscars_df.head())\n",
    "oscars_df.to_csv('data/movies_example.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:04<00:00,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 28 entries, 0 to 14\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   title         28 non-null     object\n",
      " 1   year          28 non-null     int64 \n",
      " 2   awards        28 non-null     int64 \n",
      " 3   nominations   28 non-null     int64 \n",
      " 4   best_picture  2 non-null      object\n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 1.3+ KB\n",
      "None\n",
      "                title  year  awards  nominations best_picture\n",
      "0   The King's Speech  2010       4           12         True\n",
      "1           Inception  2010       4            8          NaN\n",
      "2  The Social Network  2010       3            8          NaN\n",
      "3         The Fighter  2010       2            7          NaN\n",
      "4         Toy Story 3  2010       2            5          NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the oscar winning films from 2010 to 2011\n",
    "base_url = \"https://www.scrapethissite.com/pages/ajax-javascript/\"\n",
    "\n",
    "# Initialize a list to store the dataframes\n",
    "df_list = []\n",
    "\n",
    "for year in tqdm(range(2010, 2012)):\n",
    "\n",
    "    # Sleep for 1 to 3 seconds before each request\n",
    "    sleep(random.randint(1, 3))\n",
    "\n",
    "    # Build the url\n",
    "    parameters = \"?ajax=true&year=\" + str(year)\n",
    "    url = base_url + parameters\n",
    "\n",
    "    # Request the page\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check the status code\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error with year:\", year)\n",
    "        continue\n",
    "\n",
    "    # Format the response as a dictionary\n",
    "    response_dict = response.json()\n",
    "\n",
    "    # Format the response as a dataframe\n",
    "    year_df = pd.DataFrame(response_dict)\n",
    "    year_df[\"year\"] = year\n",
    "    df_list.append(year_df)\n",
    "\n",
    "# Concatenate and export the dataframes\n",
    "oscars_df_20102011 = pd.concat(df_list)\n",
    "print(oscars_df_20102011.info())\n",
    "print(oscars_df_20102011.head())\n",
    "oscars_df_20102011.to_csv(\"data/movies_example_20102011.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XHR-based Web Scraping: S&W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strengths**:\n",
    "\n",
    "- Flexible, efficient, and scalable:\n",
    "\n",
    "    - Can handle dynamically loaded content\n",
    "\n",
    "    - No need to parse HTML, data is already structured (usually JSON)\n",
    "\n",
    "- Sometimes more data available that what is shown on the webpage\n",
    "\n",
    "**Weaknesses**:\n",
    "\n",
    "- More difficult to implement (e.g. headers, cookies, see an example [here](https://www.youtube.com/watch?v=DqtlR0y0suo)) \n",
    "\n",
    "    - In general, requires more knowledge of the website and HTTP requests\n",
    "\n",
    "- Not always feasible (e.g., no XHR requests, tokens required)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Automated Files Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In some cases, data are provided as downloadable files (e.g. csv, pdf, xls, etc.)\n",
    "\n",
    "- It is often the case on government websites and paid databases\n",
    "\n",
    "- Some examples: [Invalsi Data](https://serviziostatistico.invalsi.it/archivio-dati/?_sft_invalsi_ss_data_collective=open-data&sf_paged=3), [Pareri Consiglio di Stato](https://www.giustizia-amministrativa.it/web/guest/dcsnprr?p_p_id=GaSearch_INSTANCE_2NDgCF3zWBwk&p_p_lifecycle=1&p_p_state=normal&p_p_mode=view&_GaSearch_INSTANCE_2NDgCF3zWBwk_javax.portlet.action=searchProvvedimenti&p_auth=yCLfGWHD), [Top Influencers on TikTok](https://starngage.com/plus/en-us/influencer/ranking/tiktok/aland-islands), [Spotify Charts](https://charts.spotify.com/charts/view/regional-global-daily/2023-09-07)\n",
    "\n",
    "- However, when the number of files is large, downloading them manually can be  time-consuming or unfeasible\n",
    "\n",
    "- Furthermore, you may want to create a standardized data collection pipeline that could be shared with others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Italian Elections Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Eligendo](https://elezioni.interno.gov.it/opendata) is the Open Data portal of the Italian Ministry of Interior. It contains data on the results of the Italian elections since 1946.\n",
    "\n",
    "![](https://github.com/pellegrinettom/intro_to_webscraping/blob/8822123725a29d33c74d5d33a03f6dbbc7a96e28/files/img/eligendo.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18-19 aprile 1948: Elezioni politiche in Italia del 1948\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Get the list of elections dates in Italy\n",
    "url = \"https://it.wikipedia.org/wiki/Calendario_delle_elezioni_in_Italia\"\n",
    "\n",
    "# Request the page\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the page\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Get the dates\n",
    "dates = [el.text for el in soup.find_all(\"ol\")[0].find_all(\"li\")]\n",
    "print(dates[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to format the dates\n",
    "import locale\n",
    "locale.setlocale(locale.LC_TIME, \"it_IT\")\n",
    "from time import strptime\n",
    "from datetime import datetime\n",
    "\n",
    "def format_date(date):\n",
    "    \"\"\"Format election dates extracted from Wikipedia to be used in the\n",
    "    Italian Elections example\"\"\"\n",
    "\n",
    "    date_str = date.split(':')[0].split() # extract the date component\n",
    "    day = date_str[0].split('-')[0]  # get the first day in case of two dates\n",
    "    formatted_month = strptime(date_str[1], '%B').tm_mon  # convert the month to a number\n",
    "    year = date.split()[-1]  # extract the year component from the end\n",
    "    dt = datetime(year=int(year), month=int(formatted_month), day=int(day))\n",
    "    formatted_date = dt.strftime('%Y%m%d')  # format the date as YYYYMMDD\n",
    "    \n",
    "    return formatted_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20130224', '20180304', '20220925']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Clean the dates with our function\n",
    "formatted_dates = [format_date(date) for date in dates]\n",
    "print(formatted_dates[-3:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:19<00:00,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         DATAELEZIONE CODTIPOELEZIONE    CIRC-REG         COLLPLURI  \\\n",
      "0  25/9/2022 00:00:00               C  PIEMONTE 1  PIEMONTE 1 - P01   \n",
      "1  25/9/2022 00:00:00               C  PIEMONTE 1  PIEMONTE 1 - P01   \n",
      "2  25/9/2022 00:00:00               C  PIEMONTE 1  PIEMONTE 1 - P01   \n",
      "3  25/9/2022 00:00:00               C  PIEMONTE 1  PIEMONTE 1 - P01   \n",
      "4  25/9/2022 00:00:00               C  PIEMONTE 1  PIEMONTE 1 - P01   \n",
      "\n",
      "                    COLLUNINOM            COMUNE  ELETTORITOT  ELETTORIM  \\\n",
      "0  PIEMONTE 1 - U03 (COLLEGNO)  CASELLE TORINESE        10851       5312   \n",
      "1  PIEMONTE 1 - U03 (COLLEGNO)  CASELLE TORINESE        10851       5312   \n",
      "2  PIEMONTE 1 - U03 (COLLEGNO)  CASELLE TORINESE        10851       5312   \n",
      "3  PIEMONTE 1 - U03 (COLLEGNO)  CASELLE TORINESE        10851       5312   \n",
      "4  PIEMONTE 1 - U03 (COLLEGNO)  CASELLE TORINESE        10851       5312   \n",
      "\n",
      "   VOTANTITOT  VOTANTIM  SKBIANCHE  VOTILISTA  \\\n",
      "0        7075      3523         84        631   \n",
      "1        7075      3523         84        476   \n",
      "2        7075      3523         84         23   \n",
      "3        7075      3523         84       1262   \n",
      "4        7075      3523         84        241   \n",
      "\n",
      "                                          DESCRLISTA   COGNOME    NOME  \\\n",
      "0                           LEGA PER SALVINI PREMIER  MACCANTI   ELENA   \n",
      "1                                       FORZA ITALIA  MACCANTI   ELENA   \n",
      "2          NOI MODERATI/LUPI - TOTI - BRUGNARO - UDC  MACCANTI   ELENA   \n",
      "3  PARTITO DEMOCRATICO - ITALIA DEMOCRATICA E PRO...  GARIGLIO  DAVIDE   \n",
      "4                          ALLEANZA VERDI E SINISTRA  GARIGLIO  DAVIDE   \n",
      "\n",
      "  LUOGONASCITA DATANASCITA SESSO  VOTICANDIDATO  \n",
      "0       TORINO  05/02/1971     F           2892  \n",
      "1       TORINO  05/02/1971     F           2892  \n",
      "2       TORINO  05/02/1971     F           2892  \n",
      "3       TORINO  03/04/1967     M           1853  \n",
      "4       TORINO  03/04/1967     M           1853  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the base URL\n",
    "base_url = \"https://dait.interno.gov.it/documenti/opendata/camera/camera-\"\n",
    "\n",
    "# Create a folder to store the data\n",
    "os.mkdir(\"data/elections\")\n",
    "\n",
    "# Download data for the last 3 elections\n",
    "for date in tqdm(formatted_dates[-3:]):\n",
    "\n",
    "    # Sleep between 5 seconds and 10 seconds\n",
    "    sleep(random.randint(5, 10))\n",
    "\n",
    "    # Build the url\n",
    "    url = base_url + date + \".zip\"\n",
    "\n",
    "    # Send the GET request (notice the importance of the headers)\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    # Check the status code\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error with date:\", date)\n",
    "        continue\n",
    "\n",
    "    # Store the content of the response\n",
    "    with open(\"data/elections/camera-\" + date + \".zip\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    # Uncompress the zip file\n",
    "    with zipfile.ZipFile(\"data/elections/camera-\" + date + \".zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"data/elections/camera-\" + date)\n",
    "\n",
    "# Explore the data-frame\n",
    "print(pd.read_csv(\"data/elections/camera-20220925/Camera_Italia_LivComune.txt\", sep=\";\").head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the folder and all its content (heavy files)\n",
    "shutil.rmtree(\"data/elections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Files Download: S&W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strengths**:\n",
    "\n",
    "- Depending on the source, data could be of higher quality:\n",
    "    - be already structured and cleaned\n",
    "    - more reliability\n",
    "\n",
    "- Makes data collection steps reproducible\n",
    "\n",
    "- Generally, requires fewer extractions\n",
    "\n",
    "**Weaknesses**:\n",
    "\n",
    "- Less control over the data collection process\n",
    "\n",
    "- Ready-to-download files rarely available for free in many domains "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping with Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Selenium?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - [```selenium```](https://www.selenium.dev/documentation/webdriver/) is an open-source software used for automating web applications for testing purposes...\n",
    " \n",
    " - ...but, it is also a powerful tool for scraping websites!\n",
    "\n",
    " - It works like a robot that can click on buttons, fill out forms, and scrape data from websites.\n",
    "\n",
    " - Useful for scraping websites where:\n",
    "    - Interaction is needed (e.g., clicking on buttons, filling out forms, scrolling)\n",
    "    - Content is loaded dynamically\n",
    "    - Authentication is required (see below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "I would suggest you to use Selenium with Chrome and the code below is written for this browser. However, you can find the list of supported browsers [here](https://www.selenium.dev/documentation/webdriver/browsers/), and there are many tutorials online on how to use it with browsers other than Chrome.\n",
    "\n",
    "When you run the code for the first time, please run the cell below to install the Chrome driver. See more details [here](https://pypi.org/project/webdriver-manager/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the Google homepage\n",
    "driver.get(\"https://www.google.com\")\n",
    "sleep(2)\n",
    "\n",
    "# Accept the cookies\n",
    "driver.find_element(By.ID, \"W0wltc\").click()\n",
    "sleep(3)\n",
    "\n",
    "# Search for \"toscrape\" in the search bar\n",
    "driver.find_element(By.NAME, \"q\").send_keys(\"the internet herokuapp\")\n",
    "sleep(3)\n",
    "\n",
    "# Click on the search button\n",
    "driver.find_element(By.NAME, \"btnK\").click()\n",
    "sleep(3)\n",
    "\n",
    "# Select the first result\n",
    "driver.find_element(By.XPATH ,'//*[@id=\"rso\"]/div[1]/div/div/div/div/div/div/div/div[1]/div/span/a').click()\n",
    "sleep(3)\n",
    "\n",
    "# Scroll down and select the \"Form Authentication\" link\n",
    "driver.execute_script(\"window.scrollTo(0, 500)\")\n",
    "sleep(3)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"content\"]/ul/li[21]/a').click()\n",
    "\n",
    "# As you can see, you can search for elements using different methods\n",
    "# Insert username and password (search by ID and NAME)\n",
    "driver.find_element(By.ID, \"username\").send_keys('tomsmith')\n",
    "sleep(2)\n",
    "driver.find_element(By.NAME, \"password\").send_keys('SuperSecretPassword!')\n",
    "sleep(2)\n",
    "# Submit (search by TAG_NAME)\n",
    "driver.find_element(By.TAG_NAME, \"button\").click()\n",
    "sleep(4)\n",
    "\n",
    "# Logout (search by XPATH)\n",
    "driver.find_element(By.XPATH, '//*[@id=\"content\"]/div/a/i').click()\n",
    "sleep(3)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selenium: S&W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Strengths**:\n",
    "\n",
    "- Flexible and powerful framework:\n",
    "\n",
    "    - Can handle dynamically loaded content\n",
    "\n",
    "    - Can interact with the webpage and perform complex actions\n",
    "\n",
    "**Weaknesses**:\n",
    "\n",
    "- More difficult to implement and maintain\n",
    "\n",
    "- Slow and resource-intensive &rarr; limited scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Approach Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be Responsible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to find an official API first or check if data is already available for download (e.g. on Kaggle, Dataverse)\n",
    "\n",
    "- Review the website's Terms of Service and [robots.txt file](https://en.wikipedia.org/wiki/Robots.txt) to check if web scraping is allowed\n",
    "\n",
    "- Comply with privacy and data protection policies when dealing with personal or sensitive data\n",
    "\n",
    "- Set a reasonable scraping rate to avoid overloading the website's server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be Smart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Is web scraping the best solution to your problem?\n",
    "\n",
    "- Check if other people have already scraped the website you are interested in.\n",
    "\n",
    "- Always evaluate the quality and the representativeness of the data you are scraping.\n",
    "\n",
    "- Test your code on a small sample of data before scaling up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Readings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Applications of Web Scraping in Economics and Finance (2022)](https://oxfordre.com/economics/display/10.1093/acrefore/9780190625979.001.0001/acrefore-9780190625979-e-652;jsessionid=0A097CCF3C3F6708C136BBEE85E7E9E9?rskey=vcsTnq#acrefore-9780190625979-e-652-bibliography-0002)\n",
    "- [Fields of Gold: Scraping Web Data for Marketing Insights (2022)](https://journals.sagepub.com/doi/pdf/10.1177/00222429221100750)\n",
    "- [Using Internet Data for Economic Research (2012)](https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.26.2.189)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
